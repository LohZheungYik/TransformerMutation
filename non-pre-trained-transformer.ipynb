{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-09-22T15:04:17.542086Z","iopub.status.busy":"2023-09-22T15:04:17.541646Z","iopub.status.idle":"2023-09-22T15:04:29.255261Z","shell.execute_reply":"2023-09-22T15:04:29.253953Z","shell.execute_reply.started":"2023-09-22T15:04:17.542051Z"},"trusted":true},"outputs":[],"source":["!pip install torchtext==0.6.0"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-22T15:04:29.260178Z","iopub.status.busy":"2023-09-22T15:04:29.259828Z","iopub.status.idle":"2023-09-22T15:04:41.096337Z","shell.execute_reply":"2023-09-22T15:04:41.095130Z","shell.execute_reply.started":"2023-09-22T15:04:29.260145Z"},"trusted":true},"outputs":[],"source":["!pip install evaluate sacrebleu"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-22T15:08:12.367887Z","iopub.status.busy":"2023-09-22T15:08:12.367496Z","iopub.status.idle":"2023-09-22T15:08:12.389818Z","shell.execute_reply":"2023-09-22T15:08:12.388611Z","shell.execute_reply.started":"2023-09-22T15:08:12.367854Z"},"trusted":true},"outputs":[],"source":["import torch\n","import spacy\n","from torchtext.data.metrics import bleu_score\n","import sys\n","import evaluate\n","\n","\n","def translate_sentence(model, sentence, fix_vocab, bug_vocab, device, max_length=100):\n","\n","    #print(fix_vocab.stoi[\"[\"])\n","    #print(bug_vocab.stoi[\"[\"])\n","\n","    tokens = sentence.split();\n","    #print(tokens)\n","\n","    tokens.insert(0, '<SOS>')\n","    tokens.append('<EOS>')\n","\n","    # Go through each fix token and convert to an index\n","    text_to_indices = [fix_vocab.stoi[token] for token in tokens]\n","    #print(text_to_indices)\n","\n","    # Convert to Tensor\n","    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n","    #print(\"sentence_tensor\")\n","    #print(sentence_tensor)\n","\n","    mutant_tokens_so_far = [bug_vocab[\"<sos>\"]]\n","    mutant_tokens = []\n","    for i in range(max_length):\n","        #trg_tensor = torch.LongTensor(mutant_tokens_so_far).unsqueeze(1).to(device)\n","\n","        with torch.no_grad():\n","            output = model(sentence_tensor, torch.LongTensor(mutant_tokens_so_far).unsqueeze(1).to(device))\n","\n","        best_guess = output.argmax(2)[-1, :].item()\n","        mutant_tokens_so_far.append(best_guess)\n","\n","        if best_guess == bug_vocab[\"<eos>\"]:\n","\n","            print(\"break\")\n","            break\n","\n","    #print(\"mutant_tokens_so_far\")\n","    #print(mutant_tokens_so_far)\n","\n","    translated_sentence = [bug_vocab.itos[idx] for idx in mutant_tokens_so_far]\n","    #print(\"transentence\")\n","    #print(translated_sentence)\n","    # remove start token\n","    return translated_sentence[1:]\n","\n","\n","def bleu(data, model, fix_vocab, bug_vocab, device):\n","    targets = []\n","    outputs = []\n","    index = 0\n","    total_score = 0\n","    \n","    with open(\"output.txt\", \"a\") as output_file:\n","        for example in data:\n","            src = vars(example)[\"f\"]\n","            trg = vars(example)[\"b\"]\n","\n","            prediction = translate_sentence(model, ' '.join(src), fix_vocab, bug_vocab, device)\n","            prediction = prediction[:-1]  # remove <eos> token\n","\n","            #print(\"prediction#\")\n","            #print(prediction)\n","            \n","            #targets.append([trg])\n","            #outputs.append(prediction)\n","\n","            source_text = ' '.join(src)\n","            target_text = ' '.join(trg)\n","            prediction_text = ' '.join(prediction)\n","\n","            prediction = [prediction_text]\n","            reference = [[target_text]]\n","            chrf = evaluate.load(\"chrf\")\n","            results = chrf.compute(predictions=prediction, references=reference)\n","            score = results[\"score\"]\n","            # Print the output\n","            print(\"Round:\", index)\n","            print(\"Score:\", score)\n","            print()\n","            \n","            # Replace print statements with code to append to the text file\n","            output_file.write(\"round : \" + str(index))\n","            output_file.write(\"source\\n\")\n","            output_file.write(\"===================\\n\")\n","            output_file.write(source_text + \"\\n\")\n","            output_file.write(\"target\\n\")\n","            output_file.write(\"===================\\n\")\n","            output_file.write(target_text + \"\\n\")\n","            output_file.write(\"prediction\\n\")\n","            output_file.write(\"===================\\n\")\n","            output_file.write(prediction_text + \"\\n\")\n","            output_file.write(\"score\\n\")\n","            output_file.write(\"===================\\n\")\n","            output_file.write(str(score) + \"\\n\")\n","\n","            total_score = total_score + score\n","            \n","            index = index + 1\n","\n","    #return bleu_score(outputs, targets)\n","    return total_score\n","\n","def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n","    print(\"=> Saving checkpoint\")\n","    torch.save(state, filename)\n","\n","\n","def load_checkpoint(checkpoint, model, optimizer):\n","    print(\"=> Loading checkpoint\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-22T15:08:16.565948Z","iopub.status.busy":"2023-09-22T15:08:16.565565Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import spacy\n","#from tranutils import translate_sentence, bleu, save_checkpoint, load_checkpoint\n","from torch.utils.tensorboard import SummaryWriter\n","from torchtext.data import Field, BucketIterator, TabularDataset\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","#tokenize\n","tokenize = lambda x: x.split()\n","\n","fix = Field(sequential=True, use_vocab=True, tokenize=tokenize, lower=True)\n","\n","bug = Field(sequential=True, use_vocab=True, tokenize=tokenize, lower=True)\n","\n","fields = {'fix': ('f', fix), 'bug': ('b', bug)}\n","\n","#load file\n","train_data, valid_data, test_data = TabularDataset.splits(path='/kaggle/input/transformernpdata/normalData', train='train.tsv', validation='validate.tsv', test='test.tsv', format='tsv', fields=fields)\n","\n","#build vocab\n","fix.build_vocab(train_data, max_size=10000,min_freq=1)\n","bug.build_vocab(train_data, max_size=10000,min_freq=1)\n","\n","fix_vocab = fix.vocab\n","bug_vocab = bug.vocab\n","\n","#split data\n","train_iterator, test_iterator = BucketIterator.splits((train_data, test_data), batch_size=2)\n","\n","class Transformer(nn.Module):\n","    def __init__(\n","        self,\n","        embedding_size,\n","        src_vocab_size,\n","        trg_vocab_size,\n","        src_pad_idx,\n","        num_heads,\n","        num_encoder_layers,\n","        num_decoder_layers,\n","        forward_expansion,\n","        dropout,\n","        max_len,\n","        device,\n","    ):\n","        super(Transformer, self).__init__()\n","        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n","        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\n","        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n","        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n","\n","        self.device = device\n","        self.transformer = nn.Transformer(\n","            embedding_size,\n","            num_heads,\n","            num_encoder_layers,\n","            num_decoder_layers,\n","            forward_expansion,\n","            dropout,\n","        )\n","        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n","        self.dropout = nn.Dropout(dropout)\n","        self.src_pad_idx = src_pad_idx\n","\n","    def make_src_mask(self, src):\n","        src_mask = src.transpose(0, 1) == self.src_pad_idx\n","\n","        # (N, src_len)\n","        return src_mask.to(self.device)\n","\n","    def forward(self, src, trg):\n","        src_seq_length, N = src.shape\n","        trg_seq_length, N = trg.shape\n","\n","        src_positions = (\n","            torch.arange(0, src_seq_length)\n","            .unsqueeze(1)\n","            .expand(src_seq_length, N)\n","            .to(self.device)\n","        )\n","\n","        trg_positions = (\n","            torch.arange(0, trg_seq_length)\n","            .unsqueeze(1)\n","            .expand(trg_seq_length, N)\n","            .to(self.device)\n","        )\n","\n","        embed_src = self.dropout(\n","            (self.src_word_embedding(src) + self.src_position_embedding(src_positions))\n","        )\n","        embed_trg = self.dropout(\n","            (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions))\n","        )\n","\n","        src_padding_mask = self.make_src_mask(src)\n","        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(\n","            self.device\n","        )\n","\n","        out = self.transformer(\n","            embed_src,\n","            embed_trg,\n","            src_key_padding_mask=src_padding_mask,\n","            tgt_mask=trg_mask,\n","        )\n","        out = self.fc_out(out)\n","        return out\n","\n","\n","# Training\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","load_model = True\n","save_model = False\n","\n","# Training hyperparameters\n","num_epochs = 0\n","learning_rate = 3e-4\n","batch_size = 32\n","\n","#data = np.load(\"losses.npy\")\n","data = np.array([[], []])\n","\n","mean_losses = []\n","epoch_counts = []\n","#last_epoch_num = data[0][-1]\n","last_epoch_num = 0\n","\n","# Model hyperparameters\n","src_vocab_size = len(fix.vocab)\n","trg_vocab_size = len(bug.vocab)\n","embedding_size = 512\n","num_heads = 8\n","\n","#TODO: CHANGE NUMBER OF LAYERS\n","num_encoder_layers = 5\n","num_decoder_layers = 5\n","dropout = 0.10\n","max_len = 100\n","forward_expansion = 4\n","src_pad_idx = bug.vocab.stoi[\"<pad>\"]\n","\n","# Tensorboard to get nice loss plot\n","writer = SummaryWriter(\"runs/loss_plot\")\n","step = 0\n","\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","    (train_data, valid_data, test_data),\n","    batch_size=batch_size,\n","    sort_within_batch=True,\n","    sort_key=lambda x: len(x.f),\n","    device=device,\n",")\n","\n","model = Transformer(\n","    embedding_size,\n","    src_vocab_size,\n","    trg_vocab_size,\n","    src_pad_idx,\n","    num_heads,\n","    num_encoder_layers,\n","    num_decoder_layers,\n","    forward_expansion,\n","    dropout,\n","    max_len,\n","    device,\n",").to(device)\n","\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer, factor=0.1, patience=10, verbose=True\n",")\n","\n","pad_idx = bug.vocab.stoi[\"<pad>\"]\n","criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n","\n","#TODO: REPLACE WITH MODEL CHECKPOINT PATH\n","if load_model:\n","    load_checkpoint(torch.load(\"/PATH/TO/TRAINING_MODEL_CHECKPOINT_.PTH.TAR FILE\"), model, optimizer)\n","\n","#sample input for simple evaluation before every training epoch\n","sentence = \"public static void main ( java.lang.String [ ] args ) throws java.io.IOException { java.lang.System.out.println ( STRING_1 ) ; TYPE_1 node = new TYPE_1 ( INT_1 ) ; node . METHOD_1 ( ) ; }\"\n","\n","\n","for epoch in range(num_epochs):\n","    print(f\"[Epoch {epoch} / {num_epochs}]\")\n","\n","    if save_model:\n","        checkpoint = {\n","            \"state_dict\": model.state_dict(),\n","            \"optimizer\": optimizer.state_dict(),\n","        }\n","        save_checkpoint(checkpoint)\n","\n","    model.eval()\n","    translated_sentence = translate_sentence(\n","        model, sentence, fix_vocab, bug_vocab, device, max_length=100\n","    )\n","\n","    #simple evaluation before every training epoch\n","    print(f\"Translated example sentence: \\n {translated_sentence}\")\n","    model.train()\n","    losses = []\n","\n","    for batch_idx, batch in enumerate(train_iterator):\n","        # Get input and targets and get to cuda\n","        inp_data = batch.f.to(device)\n","        target = batch.b.to(device)\n","\n","        # Forward prop\n","        output = model(inp_data, target[:-1, :])\n","\n","        output = output.reshape(-1, output.shape[2])\n","        target = target[1:].reshape(-1)\n","\n","        optimizer.zero_grad()\n","\n","        loss = criterion(output, target)\n","        losses.append(loss.item())\n","\n","        # Back prop\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","\n","        # Gradient descent step\n","        optimizer.step()\n","\n","        writer.add_scalar(\"Training loss\", loss, global_step=step)\n","        step += 1\n","\n","    mean_loss = sum(losses) / len(losses)\n","    print(f\"train loss: {mean_loss}\")\n","    mean_losses.append(mean_loss)\n","    epoch_counts.append(last_epoch_num + epoch + 1)\n","\n","    scheduler.step(mean_loss)\n","\n","data = np.append(data, [epoch_counts, mean_losses], axis=1)\n","np.save('losses.npy', data)\n","\n","print(\"Average Training Loss: \", mean_losses)\n","\n","\n","\n","# Plot the data\n","plt.plot(data[0], data[1])\n","plt.xlabel('Epochs')\n","plt.ylabel('Training Loss')\n","plt.title('Training Loss vs. Epochs')\n","plt.show()\n","plt.savefig(\"training_losses.png\")\n","\n","# running on entire test data takes a while\n","score = bleu(test_data, model, fix_vocab, bug_vocab, device)\n","print(f\"Average CHRF score: {score}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
